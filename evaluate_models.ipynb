{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection - Model Evaluation\n",
    "\n",
    "**Author: Brian van den Berg**\n",
    "\n",
    "This notebook tests and evaluates the performance of face detection models. The performance is measured by finding the amount of True Positives (TP), False Positives (FP) and False Negatives (FN). During execution, it is possible to view the evaluated images as long as the flag is raised and a UI is available to display opencv Matrices on. The reason for this research was out of personal interest and to support an assignment for a university class called 'Human Machine Interfaces'. Originally we only had to train a Haar Cascade classifier, but I wanted to benchmark our own model against the pre-made one and against another very popular relatively new classifier 'yunet'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To spare you the details, in the setup, the face annotations made with the opencv_annotation tool are loaded into a list of dictionaries containing the outlines of boxes. Besides that, the functions are created to perform an evaluation and if you are interested in how I solved the issue of matching annotations with detections to get the metrics for evaluation, then I would recommend you to read the following code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Annotations\n",
    "\n",
    "Change the path to the correct path, or an absolute one to the annotations file created using the 'opencv_annotation' tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Found 31 faces in 1 picture\n"
     ]
    }
   ],
   "source": [
    "annotations_path = os.path.join('evaluation', 'info.dat')\n",
    "\n",
    "# Initialize an empty list to store annotations\n",
    "annotations = []\n",
    "num_pictures = 0\n",
    "\n",
    "# Open the annotations file for reading\n",
    "with open(annotations_path, 'r') as file:\n",
    "    # Loop through the lines in the annotation file\n",
    "    for i, line in enumerate(file):\n",
    "        # Increment the line count\n",
    "        num_pictures = i + 1\n",
    "\n",
    "        # Split the line into values\n",
    "        values = line.strip().split()\n",
    "\n",
    "        # Check for the end of the annotations file\n",
    "        if len(values) == 0:\n",
    "            break\n",
    "\n",
    "        # Get the image path and number of faces in the image\n",
    "        img_path = values[0]\n",
    "        face_index = 1\n",
    "\n",
    "        # Error check the line to ensure it has the expected format\n",
    "        try:\n",
    "            num_faces = int(values[face_index])\n",
    "        except ValueError:\n",
    "            print(f'Line {i}: Warning: Skipping Line; Incorrect Format:')\n",
    "            print(f'- Line: \"{line.strip()}\"')\n",
    "            continue\n",
    "\n",
    "        # Loop through every face in the picture\n",
    "        for _ in range(num_faces):\n",
    "            # Get the variables for face 'j' in the picture\n",
    "            face_x = int(values[face_index + 1])\n",
    "            face_y = int(values[face_index + 2])\n",
    "            face_w = int(values[face_index + 3])\n",
    "            face_h = int(values[face_index + 4])\n",
    "            \n",
    "            # Increment the face_index for the next face\n",
    "            face_index += 4\n",
    "\n",
    "            # Save the variables as a facebox\n",
    "            annotation = {\n",
    "                'filename': img_path,\n",
    "                'x': face_x,\n",
    "                'y': face_y,\n",
    "                'w': face_w,\n",
    "                'h': face_h\n",
    "            }\n",
    "            # Append the facebox to the list\n",
    "            annotations.append(annotation)\n",
    "\n",
    "# Print the result for debugging\n",
    "print(f'Info: '\n",
    "      f'Found {len(annotations)} face{\"s\" if len(annotations) > 1 else \"\"} '\n",
    "      f'in {num_pictures} picture{\"s\" if num_pictures > 1 else \"\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model Evaluation\n",
    "\n",
    "As an addition to the already commented on structure, I'll also add that the 'detect_callback' arguement in 'evaluate_model' is a function callback that is called to perform face detection on an image. It is expected that the function passes a cv2.Mat type BGR image to get a list of dictionaries much like the annotations, but without 'filename' defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_center(box: dict[str, int]) -> tuple[int, int]:\n",
    "    \"\"\"Calculate the center of a bounding box.\"\"\"\n",
    "    x = round(box['x'] + box['w'] / 2)\n",
    "    y = round(box['y'] + box['h'] / 2)\n",
    "    return x, y\n",
    "\n",
    "def calculate_distance(point1: tuple[int, int], point2: tuple[int, int]) -> float:\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return distance\n",
    "\n",
    "def find_matching_annotation(annotations: list[dict[str, int]], detected: dict[str, int], error_threshold: float=0.25) -> tuple[bool, list[dict[str, any]]]:\n",
    "    \"\"\"Find and match the closest annotation to a detected face.\"\"\"\n",
    "    detected_center = calculate_center(detected)\n",
    "\n",
    "    # Find the closest label\n",
    "    minimum_distance = sys.float_info.max\n",
    "    minimum_index = None\n",
    "    for i, annotation in enumerate(annotations):\n",
    "        annotation_center = calculate_center(annotation)\n",
    "        distance = calculate_distance(detected_center, annotation_center)\n",
    "        if distance < minimum_distance:\n",
    "            minimum_distance = distance\n",
    "            minimum_index = i\n",
    "\n",
    "    # If there were no labels, return False\n",
    "    if minimum_index is None:\n",
    "        return False, annotations\n",
    "    \n",
    "    # Calculate the distance requirement based on the size of the annotation\n",
    "    annotation = annotations[minimum_index]\n",
    "    allowed_distance = (annotation['h'] / 2) * error_threshold if annotation['h'] > annotation['w'] else (annotation['w'] / 2) * error_threshold\n",
    "\n",
    "    # Check if the distance abides\n",
    "    if minimum_distance < allowed_distance:\n",
    "        # If distance is within the limit, remove the matched label\n",
    "        del annotations[minimum_index]\n",
    "        return True, annotations\n",
    "    else:\n",
    "        # If distance is too large, keep the label\n",
    "        return False, annotations\n",
    "\n",
    "def draw_rectangle(image: cv2.Mat, box: cv2.Mat, color: tuple[int, int, int]=(0, 255, 0), thickness: int=1) -> None:\n",
    "    \"\"\"Draw a rectangle on the image.\"\"\"\n",
    "    x, y, w, h = box['x'], box['y'], box['w'], box['h']\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), color, thickness)\n",
    "\n",
    "def evaluate_model(detect_callback, labels: list[dict[str, any]], detection_threshold: int=0.25, show_images: bool=False) -> dict[str, dict[str, int]]:\n",
    "    \"\"\"Evaluate a face detection model.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Iterate through every label\n",
    "    for label in labels:\n",
    "        img_path = label['filename']\n",
    "        if img_path in results:\n",
    "            continue\n",
    "\n",
    "        # Setup the evaluation metrics\n",
    "        TP, FP, FN = 0, 0, 0\n",
    "\n",
    "        # Get every annotation associated with the image\n",
    "        annotations = [annotation for annotation in labels if annotation.get('filename') == img_path]\n",
    "\n",
    "        # Detect faces in the test image using the provided callback\n",
    "        img = cv2.imread(img_path)\n",
    "        detected = detect_callback(img)\n",
    "\n",
    "        # Match every detected face with the annotations\n",
    "        for box in detected:\n",
    "            ret, annotations = find_matching_annotation(annotations, box, detection_threshold)\n",
    "            if ret:\n",
    "                # Detected face matches annotation (True Positive)\n",
    "                draw_rectangle(img, box, (0, 255, 0), 1)\n",
    "                TP += 1\n",
    "            else:\n",
    "                # Detected face does not match annotation (False Positive)\n",
    "                draw_rectangle(img, box, (0, 255, 255), 1)\n",
    "                FP += 1\n",
    "\n",
    "        # Remaining annotations were not detected (False Negative)\n",
    "        FN = len(annotations)\n",
    "        for annotation in annotations:\n",
    "            draw_rectangle(img, annotation, (0, 0, 255), 1)\n",
    "\n",
    "        # Display the marked test image if requested\n",
    "        if show_images:\n",
    "            cv2.imshow(\"Image\", img)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        # Add the metrics to the result\n",
    "        results[img_path] = {'TP': TP, 'FP': FP, 'FN': FN}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Yunet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evaluation\\\\group.jpg': {'TP': 29, 'FP': 0, 'FN': 2}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the yunet model\n",
    "yunet_model_path = os.path.join('yunet', 'models', 'face_detection_yunet_2023mar.onnx')\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model=      yunet_model_path,\n",
    "    config=     \"\",\n",
    "    input_size= (0, 0)\n",
    ")\n",
    "\n",
    "# Define a detect callback function that delivers the result in the required format\n",
    "def yunet_detect(img: cv2.Mat) -> list[dict[str, int]]:\n",
    "    yunet.setInputSize((img.shape[1], img.shape[0]))\n",
    "    _, faces = yunet.detect(img)\n",
    "    result = []\n",
    "    for face in faces:\n",
    "        result.append({\n",
    "            'x': int(face[0]),\n",
    "            'y': int(face[1]),\n",
    "            'w': int(face[2]),\n",
    "            'h': int(face[3])\n",
    "        })\n",
    "    return result\n",
    "\n",
    "# Perform the evaluation on yunet\n",
    "evaluation_results = evaluate_model(yunet_detect, annotations, show_images=True)\n",
    "print(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
